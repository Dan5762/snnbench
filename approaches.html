
<!-- index.html -->
<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="apple-touch-icon" sizes="57x57" href="images/apple-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="60x60" href="images/apple-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="72x72" href="images/apple-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="76x76" href="images/apple-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="114x114" href="images/apple-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="120x120" href="images/apple-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="144x144" href="images/apple-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="152x152" href="images/apple-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="images/apple-icon-180x180.png">
    <link rel="icon" type="image/png" sizes="192x192"  href="images/android-icon-192x192.png">
    <link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="96x96" href="images/favicon-96x96.png">
    <link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
  
    <link rel="manifest" href="manifest.json">
  
    <meta name="msapplication-TileColor" content="#ffffff">
    <meta name="msapplication-TileImage" content="/ms-icon-144x144.png">
    <meta name="theme-color" content="#ffffff">

    <title>SNN Home</title>
    <meta name="description" content="Public Benchmarking for Spiking Neural Networks">
    <meta name="author" content="Daniel Long">
    <meta property="og:title" content="SNN Benchmarks">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://snnbench.com">
    <meta property="og:description" content="Public Benchmarking for Spiking Neural Networks">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-YQ1SQVV1YL"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-YQ1SQVV1YL');
    </script>

    <link rel="stylesheet" type="text/css" href="styles.css">
  </head>
  <body>
    <header>
      <h1>Spiking Neural Network Benchmarks</h1>
      <nav>
        <ul>
          <li><a href="index.html">Home</a></li>
          <li><a href="approaches.html">Approaches</a></li>
          <li><a href="contact.html">Contact</a></li>
        </ul>
      </nav>
    </header>
    <main>
      <h2>Approaches</h2>
      <p>There are many approaches to implementing learning within spiking neural networks, determining an ideal categorisation will likely be an ongoing challenge. The results presented on this site have been organised into following categories:</p>
      <h3 class="pad-above">Supervised Learning</h3>
      <p>These approaches use an error signal to directly modify weights, typically employing backpropagation or other direct feedback mechanisms.</p>
      <h3 class="pad-above">Unsupervised Learning</h3>
      <p>These methods learn from input data without labeled examples, using local learning rules like STDP to adjust synaptic weights based on spike timing.</p>
      <h3 class="pad-above">Reinforcement Learning</h3>
      <p>These approaches combine spike-based learning with reinforcement signals, using reward-modulated STDP or policy gradient methods to optimize behavior based on rewards.</p>
      <h3 class="pad-above">Hybrid Learning</h3>
      <p>These methods integrate supervised and unsupervised learning, often using unsupervised pre-training to learn features, followed by supervised fine-tuning for specific tasks.</p>
      <h3 class="pad-above">Spike-Based Backpropagation Variants</h3>
      <p>These variants adapt traditional backpropagation for spiking neurons, using surrogate gradients to handle the non-differentiable nature of spiking activity.</p>
      <h3 class="pad-above">Reservoir Computing and Liquid State Machines</h3>
      <p>These approaches use a fixed, randomly connected network (reservoir) to process inputs, training only the readout layer to leverage the reservoirâ€™s dynamic response.</p>
    </main>
  </body>
</html>